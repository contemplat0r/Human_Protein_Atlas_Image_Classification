{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import skimage\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import load_data\n",
    "import model\n",
    "#import first_exp_model_with_pytorch as dloader\n",
    "#import first_exp_model_with_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('Solarize_Light2')\n",
    "%matplotlib inline\n",
    "importlib.reload(load_data)\n",
    "importlib.reload(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = H = 256\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "LABEL_NAMES_MAP = {\n",
    "    0:  \"Nucleoplasm\",  \n",
    "    1:  \"Nuclear membrane\",   \n",
    "    2:  \"Nucleoli\",   \n",
    "    3:  \"Nucleoli fibrillar center\",   \n",
    "    4:  \"Nuclear speckles\",\n",
    "    5:  \"Nuclear bodies\",   \n",
    "    6:  \"Endoplasmic reticulum\",   \n",
    "    7:  \"Golgi apparatus\",   \n",
    "    8:  \"Peroxisomes\",   \n",
    "    9:  \"Endosomes\",   \n",
    "    10:  \"Lysosomes\",   \n",
    "    11:  \"Intermediate filaments\",   \n",
    "    12:  \"Actin filaments\",   \n",
    "    13:  \"Focal adhesion sites\",   \n",
    "    14:  \"Microtubules\",   \n",
    "    15:  \"Microtubule ends\",   \n",
    "    16:  \"Cytokinetic bridge\",   \n",
    "    17:  \"Mitotic spindle\",   \n",
    "    18:  \"Microtubule organizing center\",   \n",
    "    19:  \"Centrosome\",   \n",
    "    20:  \"Lipid droplets\",   \n",
    "    21:  \"Plasma membrane\",   \n",
    "    22:  \"Cell junctions\",   \n",
    "    23:  \"Mitochondria\",   \n",
    "    24:  \"Aggresome\",   \n",
    "    25:  \"Cytosol\",   \n",
    "    26:  \"Cytoplasmic bodies\",   \n",
    "    27:  \"Rods & rings\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(image, labels):\n",
    "    f, (ax1,ax2,ax3,ax4) = plt.subplots(1, 4, figsize=(25,25), sharey=True)\n",
    "\n",
    "    title = ''\n",
    "    #npimage = image.numpy()[:, ::-1, :, :]\n",
    "    #print(image.size())\n",
    "    #print(labels)\n",
    "    #print(labels.size())\n",
    "    #print(image)\n",
    "    npimage = image.numpy()\n",
    "    nplabels = labels.numpy()\n",
    "    print(npimage.shape)\n",
    "    print(nplabels)\n",
    "    #transposed_npimage = np.transpose(npimage, (1, 2, 0))\n",
    "    \n",
    "    #print(transposed_npimage.shape)\n",
    "    \n",
    "    #labels =sample['target']\n",
    "                \n",
    "    #for i, label in enumerate(LABELS):\n",
    "    #    if labels[i] == 1:\n",
    "    #        if title == '':\n",
    "    #            title += label\n",
    "    #        else:\n",
    "    #            title += \" & \" + label\n",
    "    \n",
    "    #for i, label in enumerate(nplabels[0]):\n",
    "    for i, label in enumerate(nplabels):\n",
    "        if label == 1:\n",
    "            if title == '':\n",
    "                title += LABEL_NAMES_MAP[i]\n",
    "            else:\n",
    "                title += \" & \" + LABEL_NAMES_MAP[i]\n",
    "    \n",
    "    ax1.imshow(npimage[0,:,:],cmap=\"hot\")\n",
    "    ax1.set_title('Red')\n",
    "    ax2.imshow(npimage[1,:,:],cmap=\"copper\")\n",
    "    ax2.set_title('Green')\n",
    "    ax3.imshow(npimage[2,:,:],cmap=\"bone\")\n",
    "    ax3.set_title('Blue')\n",
    "    ax4.imshow(npimage[3,:,:],cmap=\"afmhot\")\n",
    "    ax4.set_title('Yellow')\n",
    "    f.suptitle(title, fontsize=20, y=0.62)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HPA_ConvNeuralNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(HPA_ConvNeuralNet, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(4, 8, 5, 1, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(8, 16, 5, 1, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.drop_out = nn.Dropout()\n",
    "        self.out1 = nn.Linear(int(16 * W/4 * H/4), 900)\n",
    "        self.out2 = nn.Linear(900, 28)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        output = self.drop_out(x)\n",
    "        output = self.out1(x)\n",
    "        output = self.out2(output)\n",
    "        #return output, x\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_loaders(dataset, valid_train_ratio=0.6):\n",
    "    dataset_size = len(dataset)\n",
    "    print(\"dataset_size: \", dataset_size)\n",
    "    #train_subset_size = int(valid_train_ratio * dataset_size)\n",
    "    validation_subset_size = int(dataset_size * (1 - valid_train_ratio))\n",
    "    print(\"validation_subset_size: \", validation_subset_size)\n",
    "\n",
    "    indices = list(range(dataset_size))\n",
    "    validation_indices = np.random.choice(indices, size=validation_subset_size, replace=False)\n",
    "    train_indices = list(set(indices) - set(validation_indices))\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    validation_sampler = SubsetRandomSampler(validation_indices)\n",
    "    \n",
    "    dataset_sizes = {\n",
    "            'train': len(train_indices),\n",
    "            'validation': len(validation_indices)\n",
    "        }\n",
    "\n",
    "    train_loader = data.DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=1, sampler=train_sampler)\n",
    "    validation_loader = data.DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=1, sampler=validation_sampler)\n",
    "\n",
    "    loaders = {\n",
    "            'train': train_loader,\n",
    "            'validation': validation_loader\n",
    "        }\n",
    "\n",
    "    return loaders, dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, validation_loader, validation_dataset_size):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        i = 0\n",
    "        for images, labels in validation_loader:\n",
    "            images = images.to(device, dtype=torch.float)\n",
    "            print(\"images.size(): \", images.size())\n",
    "            \n",
    "            labels = labels.to(device, dtype=torch.uint8)\n",
    "            \n",
    "            print(\"labels.size(): \", labels.size())\n",
    "            print(\"labels:\\n\", labels)\n",
    "            print(\"labels.dtype: \", labels.dtype)\n",
    "            outputs = model(images)\n",
    "            #outputs = model(images)[0]\n",
    "            print(\"outputs.size(): \", outputs.size())\n",
    "            print(\"outputs:\\n\", outputs)\n",
    "            print(\"outputs.data:\\n\", outputs.data)\n",
    "            #_, predicted = torch.max(outputs.data, 1)\n",
    "            #pred, predicted = torch.max(outputs.data, 1)\n",
    "            predicted = outputs > 0\n",
    "            #predicted = outputs.to(device, dtype=torch.int64)\n",
    "            predicted = outputs.to(device, dtype=torch.uint8)\n",
    "            print(\"predicted.size(): \", predicted.size())\n",
    "            print(\"predicted:\\n\", predicted)\n",
    "            #print(\"predicted.size(): \", predicted.size())\n",
    "            #print(\"pred:\\n\", pred)\n",
    "            total += labels.size(0)\n",
    "            print(\"labels.sum(): \", labels.sum())\n",
    "            print(\"predicted.sum():\", predicted.sum())\n",
    "            print(\"predicted == labels:\\n\", predicted == labels)\n",
    "            correct += torch.sum((predicted == labels).all(1))\n",
    "            i += 1\n",
    "            if i == 10:\n",
    "                break\n",
    "        accuracy = correct / total\n",
    "        print(\n",
    "                \"Validation accuracy of the model on the {} validation images: {} \".format(\n",
    "                    validation_dataset_size,\n",
    "                    accuracy\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_data.load_text_data('../input/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_data.HumanProteinAtlasDataset(\n",
    "    train_df,\n",
    "    transform=load_data.transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_loaders, dataset_sizes = prepare_loaders(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_loader = dloader.data.DataLoader(dataset, batch_size=10, shuffle=True, num_workers=1)\n",
    "#train_loader = load_data.data.DataLoader(train_dataset, batch_size=40, shuffle=True, num_workers=1)\n",
    "train_loader = dataset_loaders['train']\n",
    "validation_loader = dataset_loaders['validation']\n",
    "train_dataset_size = dataset_sizes['train']\n",
    "validation_dataset_size = dataset_sizes['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset_size)\n",
    "print(validation_dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = train_iter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"images shape on batch size = {}\".format(images.size()))\n",
    "print(\"labels shape on batch size = {}\".format(labels.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid = torchvision.utils.make_grid(images)\n",
    "#grid = torchvision.utils.make_grid([image[0] for image in images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(grid)\n",
    "#plt.axis('off')\n",
    "#plt.title(labels.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(images[0], labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpa_conv_neural_net = HPA_ConvNeuralNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpa_conv_neural_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion=nn.BCEWithLogitsLoss(reduction='sum')\n",
    "#optimizer = optim.SGD(hpa_conv_neural_net.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.Adam(hpa_conv_neural_net.parameters(), lr=0.001, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "since = time.time()\n",
    "for epoch in range(2):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        \n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device, dtype=torch.float), labels.to(device, dtype=torch.float)\n",
    "        print(\"inputs.size(): \", inputs.size())\n",
    "        print(\"inputs:\\n\", inputs)\n",
    "        #labels = labels[0]\n",
    "        print(\"labels.size(): \", labels.size())\n",
    "        print(\"labels: \", labels)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = hpa_conv_neural_net(inputs)\n",
    "        print(\"type(outputs): \", type(outputs))\n",
    "        print(\"outputs.size(): \", outputs.size())\n",
    "        print(\"outputs:\\n\", outputs)\n",
    "        #loss = criterion(outputs, labels.float())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 200 == 199:\n",
    "            print(\"[%d, %5d, loss: %.3f]\" % (epoch + 1, i + 1, running_loss / 20.0))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "print(\"Finished Training\")\n",
    "time_elapsed = time.time() - since\n",
    "print(\n",
    "    'Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_accuracy = validate(hpa_conv_neural_net, validation_loader, validation_dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
